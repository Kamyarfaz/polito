{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import LongType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data and output folders - SPARK SQL\n",
    "inUsers = \"Users.txt\"\n",
    "inSongs = \"Songs.txt\"\n",
    "inListenToSongs = \"ListenToSongs.txt\"\n",
    "\n",
    "outputPathPart1 = \"outPart1SQL/\"\n",
    "outputPathPart2 = \"outPart2SQL/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the input data\n",
    "usersDF = spark.read.load(inUsers,\\\n",
    "                            format=\"csv\",\\\n",
    "                            header=False,\\\n",
    "                            inferSchema=True)\\\n",
    ".withColumnRenamed(\"_c0\", \"UID\")\\\n",
    ".withColumnRenamed(\"_c1\", \"Name\")\\\n",
    ".withColumnRenamed(\"_c2\", \"Surname\")\\\n",
    ".withColumnRenamed(\"_c3\", \"Gender\")\\\n",
    ".withColumnRenamed(\"_c4\", \"YearOfBirth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songsDF = spark.read.load(inSongs,\\\n",
    "                            format=\"csv\",\\\n",
    "                            header=False,\\\n",
    "                            inferSchema=True)\\\n",
    ".withColumnRenamed(\"_c0\", \"SID\")\\\n",
    ".withColumnRenamed(\"_c1\", \"Title\")\\\n",
    ".withColumnRenamed(\"_c2\", \"MusicGenre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listenDF = spark.read.load(inListenToSongs,\\\n",
    "                            format=\"csv\",\\\n",
    "                            header=False,\\\n",
    "                            inferSchema=True)\\\n",
    ".withColumnRenamed(\"_c0\", \"SID\")\\\n",
    ".withColumnRenamed(\"_c1\", \"UID\")\\\n",
    ".withColumnRenamed(\"_c2\", \"StartTimestamp\")\\\n",
    ".withColumnRenamed(\"_c3\", \"EndTimestamp\")\\\n",
    ".cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Associate the input dataframes to temporary tables\n",
    "usersDF.createOrReplaceTempView(\"users\")\n",
    "songsDF.createOrReplaceTempView(\"songs\")\n",
    "listenDF.createOrReplaceTempView(\"listen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the songs never listened to by young users in the last two years\n",
    "midsIncomesDF = spark.sql(\"\"\"SELECT SID\n",
    "FROM songs \n",
    "     LEFT ANTI JOIN\n",
    "     (SELECT SID\n",
    "      FROM listen, users \n",
    "      WHERE listen.UID=users.UID\n",
    "      AND StartTimestamp>='2018/09/14' \n",
    "      AND StartTimestamp<='2020/09/13'\n",
    "      AND YearOfBirth>1999) listenByYoung\n",
    "     ON songs.SID=listenByYoung.SID\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#midsIncomesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the result\n",
    "midsIncomesDF.write.csv(outputPathPart1,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a UDF that extract the year part from Date\n",
    "def yearFunc(timestamp):\n",
    "    return int(timestamp.split(\"/\")[0])\n",
    "    \n",
    "spark.udf.register(\"yearFunc\", yearFunc, LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute for each song its popularity in each year, \n",
    "# i.e., the number of distinct users who listened to it in that year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates. We are interested in the number of distinct users for each (song,year)\n",
    "sidYearDistinctUserDF = spark.sql(\"\"\"\n",
    "SELECT DISTINCT SID,\n",
    "yearFunc(StartTimestamp) as Year,\n",
    "UID \n",
    "FROM listen\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Associate sidYearDistinctUserDF to a temporary table\n",
    "sidYearDistinctUserDF.createOrReplaceTempView(\"listenNoDuplicates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of distinct user for each pair (sid,year)\n",
    "sidYearPopularityDF = spark.sql(\"\"\"SELECT SID, Year, COUNT(*) as YearlyPopularity\n",
    "FROM listenNoDuplicates\n",
    "GROUP BY SID, Year\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sidYearPopularityDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Associate sidYearPopularityDF to a temporary table\n",
    "sidYearPopularityDF.createOrReplaceTempView(\"SIDYearlyPopularity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the highest yearly popularity for each song\n",
    "sidMaxPopDF = spark.sql(\"\"\"SELECT SID, MAX(YearlyPopularity) as MaxPop\n",
    "FROM SIDYearlyPopularity\n",
    "GROUP BY SID\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sidMaxPopDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Associate sidMaxPopDF to a temporary table\n",
    "sidMaxPopDF.createOrReplaceTempView(\"SIDMaxYearlyPopularity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select for each song the first year \n",
    "# associated with its highest popularity\n",
    "sidFirstYearMaxPopDF = spark.sql(\"\"\"SELECT SIDYearlyPopularity.SID, MIN(Year) \n",
    "FROM SIDYearlyPopularity,SIDMaxYearlyPopularity\n",
    "WHERE SIDYearlyPopularity.SID=SIDMaxYearlyPopularity.SID\n",
    "AND SIDYearlyPopularity.YearlyPopularity=SIDMaxYearlyPopularity.MaxPop\n",
    "GROUP BY SIDYearlyPopularity.SID\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sidFirstYearMaxPopDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the result\n",
    "sidFirstYearMaxPopDF.write.csv(outputPathPart2,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
