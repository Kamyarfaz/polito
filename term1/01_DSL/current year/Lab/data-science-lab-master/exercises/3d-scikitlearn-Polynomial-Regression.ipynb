{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Polynomial regression\n",
    "## 1.1 Try linear regression on non-linear data\n",
    "### a. Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Build dataset with non-linear data\n",
    "def make_nonlinear_data(num_samples):\n",
    "    np.random.seed(123456)\n",
    "    # Make data\n",
    "    noise = np.random.normal(0,1, num_samples) # gaussian data, mean=0, std=1\n",
    "    x = 5*np.random.rand((num_samples))        # random samples between 0 and 5\n",
    "    y = 4*np.cos(x) + noise                    # generate random y, non-linear\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the regression line\n",
    "def plot_regression_line(reg, label=\"Regression line\", start=0, stop=5):\n",
    "    # For plots generate 50 linearly spaced samples between start and stop\n",
    "    x_reg = np.linspace(start, stop, 50)\n",
    "    y_reg = reg.predict(x_reg[:, np.newaxis])\n",
    "    plt.plot(x_reg, y_reg, label=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print R2 score, computed with cross-validation\n",
    "def print_score(reg, x, y):\n",
    "    y_pred = cross_val_predict(reg, x[:, np.newaxis], y, cv=5)\n",
    "    print(f\"R2: {r2_score(y, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Run regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate data\n",
    "x, y = make_nonlinear_data(1000)\n",
    "\n",
    "# Regression (computed with all data)\n",
    "reg = LinearRegression()\n",
    "reg.fit(x[:,np.newaxis], y)\n",
    "\n",
    "# Plots\n",
    "plt.scatter(x, y, s=1, c='grey')\n",
    "plot_regression_line(reg)\n",
    "plt.show()\n",
    "\n",
    "# Score (using cross validation)\n",
    "print_score(reg, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Linear regression with polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Generate data\n",
    "x, y = make_nonlinear_data(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Let's define a pipeline with PolynomialFeatures and LinearRegression**\n",
    "- Try width degree = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression (computed with all data)\n",
    "reg = ??\n",
    "reg.fit(x[:,np.newaxis], y)\n",
    "\n",
    "# Plots\n",
    "plt.scatter(x, y, s=1, c='grey')\n",
    "plot_regression_line(reg)\n",
    "plt.show()\n",
    "\n",
    "# Score (using cross validation)\n",
    "print_score(reg, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Overfitting and regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to fit, plot the regression line, display the R2\n",
    "# of a polynomial regression with parametric degree\n",
    "def polynomial_regression(x, y, degree):\n",
    "    # Regression (computed with all data)\n",
    "    reg = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "    reg.fit(x[:,np.newaxis], y)\n",
    "\n",
    "    # Plots\n",
    "    plt.scatter(x, y, s=1, c='grey')\n",
    "    plot_regression_line(reg, label=\"d=\"+str(degree))\n",
    "\n",
    "    # Score (using cross validation)\n",
    "    print(\"Degree = %d\" % degree)\n",
    "    print_score(reg, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Let's try some different polynomial degrees with 1000 data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x, y = make_nonlinear_data(1000)\n",
    "polynomial_regression(x, y, 2)\n",
    "polynomial_regression(x, y, 3)\n",
    "polynomial_regression(x, y, 15)    # Seems to work well\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Overfitting example\n",
    "- Reduce data to only 60 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = make_nonlinear_data(60)\n",
    "polynomial_regression(x, y, 2)\n",
    "polynomial_regression(x, y, 3)\n",
    "polynomial_regression(x, y, 15)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ** Degree 3 is quite good, but degree 15 this time overfits **\n",
    "- ** Higher degree is not always better!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Regularization to tackle overfitting\n",
    "- Define a new method for applying **Lasso** to our non-linear dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Here we apply Lasso as regularized linear regression\n",
    "def polynomial_regression_regularized(x, y, degree):\n",
    "    # Regression (computed with all data)\n",
    "    # Tolerance is the minimum scoring (R2) improvement for the algorithm to continue the optimization\n",
    "    # if it is too low, the algorithm optimization may not converge\n",
    "    reg = make_pipeline(PolynomialFeatures(degree), Lasso(alpha=0.5, tol=0.2))\n",
    "    reg.fit(x[:,np.newaxis], y)\n",
    "\n",
    "    # Plots\n",
    "    plt.scatter(x, y, s=1, c='grey')\n",
    "    plot_regression_line(reg, label=\"Lasso, d=\" + str(degree))\n",
    "\n",
    "    # Score (using cross validation)\n",
    "    print(\"Lasso, degree = %d\" % degree)\n",
    "    print_score(reg, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x, y = make_nonlinear_data(60)\n",
    "poly = polynomial_regression(x, y, 15)\n",
    "poly_reg = polynomial_regression_regularized(x, y, 15)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso achieves better R2 rather than common linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Grid-search to select model hyperparameters\n",
    "## Important:\n",
    "- Test data must not be used during hyperparameter selection\n",
    "- If you use it to tune parameters, there is the risk of overfitting the test-set\n",
    "- 2 possibilities:\n",
    "- a. Divide your data in: training, validation and test set\n",
    "    - **Training**: used to train model configurations\n",
    "    - **Validation**: used to verify which model configuration is the best\n",
    "    - **final model** = train the selected configuration on (Training U Validation)\n",
    "    - **Test**: used to asses the quality of the final\n",
    "- b. Divide your data in: training test set\n",
    "    - **Training**: perform cross-validation on this set to select the best model based on some score\n",
    "    - **final model** = train the selected configuration on the complete training set\n",
    "    - **Test**: used to asses the quality of the final model\n",
    "- Using cross-validation (b) is **slower**, but **less prone to overfitting**, because it chooses the best model based on a score that is averaged between multiple train-validation splits.\n",
    "\n",
    "### 3.1 Let's try to implement the second version (b).\n",
    "### First, make some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Build dataset with non-linear data\n",
    "def make_nonlinear_data2(num_samples):\n",
    "    np.random.seed(123456)\n",
    "    # Make data\n",
    "    err = np.random.normal(0,6, num_samples)\n",
    "    x = 5*np.random.rand((num_samples))        \n",
    "    y = np.exp(x) + err     \n",
    "    return x, y\n",
    "\n",
    "# Generate dataset\n",
    "x, y = make_nonlinear_data2(100)\n",
    "plt.scatter(x, y, s=10, c='grey')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Now configure the grid-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression model that we want to configure:\n",
    "reg = make_pipeline(PolynomialFeatures(), Lasso(tol=0.4))\n",
    "\n",
    "# Parameter grid\n",
    "# Specify in a dictionary a key for each parameter to be configured and the list of its values\n",
    "# The keys in the dictionary must match the parameter names of the model constructor\n",
    "# In the case of pipelines (chain of models) specify the name of the model, two underscores\n",
    "# and the parameter name.\n",
    "# Example:\n",
    "param_grid = {'polynomialfeatures__degree':list(range(2,20,2)), \n",
    "              'lasso__alpha':[1e-2, 0.1, 1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "    \n",
    "# Configure grid search with the model and the parameter grid\n",
    "# The best model is computed based on the scoring function (R2) and with cross-validation (cv)\n",
    "gridsearch = GridSearchCV(reg, param_grid, scoring='r2', cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Apply cross-validation on training data to find the best configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make data (training, test set)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y)\n",
    "\n",
    "# Optimize the model parameters with cross-validation on the training data\n",
    "# The best model configuration is the one with highest average R2 score\n",
    "# (R2 is averaged among all the cross-validation partitions)\n",
    "res = gridsearch.fit(x_train[:, np.newaxis], y_train)\n",
    "\n",
    "# Print result\n",
    "print(\"Best model configuration is:\")\n",
    "print(res.best_params_)\n",
    "print(\"with R2=%.2f\" % res.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Train the selected configuration on the complete training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the best model configuration\n",
    "final_model = res.best_estimator_\n",
    "\n",
    "# Plot result\n",
    "plt.scatter(x_test, y_test, s=10, c='grey')\n",
    "plot_regression_line(final_model)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_test_pred = final_model.predict(x_test[:,np.newaxis])\n",
    "r2 = r2_score(y_test, y_test_pred)\n",
    "print(\"R2 (test set): %.2f\" % r2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
