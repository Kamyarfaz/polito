{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "seed = 1234"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Classification and hold-out\n",
    "## 1.1 Load 'abalone' dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are the attribute names, attribute types, the measurement units and a brief description.\n",
    "\n",
    "Your goal is to predict the sex of the abalone, based on the rest of the available information.\n",
    "\n",
    "```\n",
    "Name / Data Type / Measurement Unit / Description\n",
    "-----------------------------\n",
    "Length / continuous / mm / Longest shell measurement\n",
    "Diameter / continuous / mm / perpendicular to length\n",
    "Height / continuous / mm / with meat in shell\n",
    "Whole weight / continuous / grams / whole abalone\n",
    "Shucked weight / continuous / grams / weight of meat\n",
    "Viscera weight / continuous / grams / gut weight (after bleeding)\n",
    "Shell weight / continuous / grams / after being dried\n",
    "Rings / integer / -- / +1.5 gives the age in years \n",
    "Sex / nominal / -- / 2: M, 0: F, and 1: I (infant)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labels\n",
    "df = pd.read_csv(\"abalone.csv\", sep=\" \", header=None, names=[\"length\", \"diameter\", \"height\", \"w_weight\", \"s_weight\", \"v_weight\", \"sh_weight\", \"rings\", \"sex\"])\n",
    "X = ??\n",
    "y_truth = ??\n",
    "\n",
    "# Count items for each class\n",
    "??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Create train and test splits\n",
    "- Use the train_test_split() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate data into training and test set\n",
    "# Default test_size = 0.25\n",
    "X_train, X_test, y_train, y_test = ??\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Train classifier and make predictions\n",
    "- Use Gaussian Naive Bayes classifier\n",
    "- Random state to make results repeatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ??\n",
    "\n",
    "y_test_pred = ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Evaluate the results\n",
    "- Evaluation using accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "# Compute accuracy\n",
    "acc = ??\n",
    "print(f\"Accuracy = {acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Accuracy** seems good, but if we look at the scores separately for each class..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Precision, recall, f1, support: for each class\n",
    "p, r, f1, support = precision_recall_fscore_support(y_test, y_test_pred)\n",
    "\n",
    "for c in range(p.shape[0]):\n",
    "    print(f\"\\nClass {c}:\")\n",
    "    print(f\"number of items: {support[c]}\")\n",
    "    print(f\"p = {p[c]:.2f}\")\n",
    "    print(f\"r = {r[c]:.2f}\")\n",
    "    print(f\"f1 = {f1[c]:.2f}\")\n",
    "\n",
    "# Macro average f1\n",
    "macro_f1 = ?? \n",
    "    \n",
    "# This score is important when you have class imbalancing\n",
    "print(f\"\\nF1, macro-average: {macro_f1:2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Accuracy was good because of class imbalancing\n",
    "- The **minority class** (c2) has a very low recall\n",
    "- Indeed, the **macro-averaged** F1 is quite low.\n",
    "\n",
    "### Let's verify this with a confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Build the confusion matrix\n",
    "conf_mat = confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the result\n",
    "label_names = np.arange(p.shape[0])\n",
    "conf_mat_df = pd.DataFrame(conf_mat, index = label_names, columns = label_names)\n",
    "conf_mat_df.index.name = 'Actual'\n",
    "conf_mat_df.columns.name = 'Predicted'\n",
    "sns.heatmap(conf_mat_df, annot=True, cmap='GnBu', \n",
    "            annot_kws={\"size\": 16}, fmt='g', cbar=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Cross-Validation\n",
    "##  2.1 With kfold.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "# K-Fold with 5 splits\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "print(\"Scores for each kfold iteration.\")\n",
    "i = 0\n",
    "for train_indices, test_indices in kfold.split(X, y_truth):\n",
    "    # Prepare splits\n",
    "    X_train = ??\n",
    "    y_train = ??\n",
    "    X_test = ??\n",
    "    y_test = ??\n",
    "    \n",
    "    # Train and evaluate\n",
    "    clf = GaussianNB()\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_test_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Compute macro average f1\n",
    "    _, _, f1, _ = precision_recall_fscore_support(y_test, y_test_pred)\n",
    "    macro_f1 = f1.mean()\n",
    "    \n",
    "    print(f\"Iteration {i}. macro-f1 = {macro_f1}\")\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 With cross_val_score()\n",
    "- Use scoring = 'f1_macro'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GaussianNB()\n",
    "f1_cv = ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Macro-f1 for each iteration: {f1_cv}\")\n",
    "mean_macro_f1 = f1_cv.mean()\n",
    "std_macro_f1 = f1_cv.std() * 2\n",
    "print(f\"Macro-f1 (statistics): {mean_macro_f1:.2f} (+/- {std_macro_f1:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Leave-One-Out and scoring: cross_val_predict()\n",
    "\n",
    "- The previous approach (average of F1 for each iteration) cannot be used with leave one out. \n",
    "    - Iteration 0: y_test = [1] -> F1?\n",
    "    - Iteration 1: y_test = [0] -> F1?\n",
    "    - ...\n",
    "    - Iteration 2: y_test = [1] -> F1?\n",
    "- When test set has only 1 sample, F1, precision and recall cannot be properly computed.\n",
    "- The following solution trains N models with leave one out, fits them on test data to obtain the vector y_pred (each model predicts 1 single value inside y_pred). Finally, it computes a single score by comparing y_pred with y_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "clf = GaussianNB()\n",
    "y_pred = cross_val_predict(clf, X, y_truth, cv=LeaveOneOut())\n",
    "_, _, f1_loo, _ = precision_recall_fscore_support(y_truth, y_pred)\n",
    "macro_f1_loo = f1_loo.mean()\n",
    "print(f\"F1, for each class: {f1_loo}\")\n",
    "print(f\"Macro-f1 = {macro_f1_loo:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
